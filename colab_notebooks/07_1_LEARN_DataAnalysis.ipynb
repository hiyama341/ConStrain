{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07_1_LEARN-DataAnalysisML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/hiyama341/ConStrain/blob/main/colab_notebooks/07_1_LEARN_DataAnalysis.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can import the data repositoru like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ConStrain_on_google_colab' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# cloning the data repo\n",
    "!git clone https://github.com/hiyama341/ConStrain_on_google_colab.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Intro\n",
    "In this study, we aim to use machine learning techniques to predict the best promoter-gene combinations. Machine learning is a powerful tool that allows us to analyze large and complex datasets, identify patterns and make predictions. We will use various machine learning algorithms through the package H2O. \n",
    "\n",
    "The machine learning models will be trained on the data from the experiments from[06_1_TEST_LibraryCharacterisation](../colab_notebooks/06_1_TEST_LibraryCharacterisation.ipynb), and will learn to predict the best promoter-gene combination based on the observed phenotype and genotype. This will enable us to identify the combination of genes and promoters that result in the highest level of expression or activity, without the need for additional experimentation.\n",
    "\n",
    "Ultimately, the use of machine learning to predict the best promoter-gene combination will greatly improve the efficiency and allowing us to identify the best combination in a shorter time and with fewer resources.\n",
    "\n",
    "\n",
    "\n",
    "In this notebook we continue the workflow by using Machine Learning to predict the best promoter:gene combinations of the remaining library that was generated in [05_1_BUILD_CombinatorialLibrary_AllStrain](../colab_notebooks/05_1_BUILD_CombinatorialLibrary_AllStrains.ipynb) and analyzed in [06_1_TEST_LibraryCharacterisation](../colab_notebooks/06_1_TEST_LibraryCharacterisation.ipynb). \n",
    "\n",
    "## Project overview - Use ML to predicte best promoter:gene combinations \n",
    "\n",
    "**Hypothesis**\n",
    "1. Specific combinations of CPR / G8H homologs and corresponding expression levels can remove the G8H bottleneck in the Strictosidine pathway\n",
    "\n",
    "Aim: To test the hypothesis\n",
    "\n",
    "Tasks\n",
    "1. dgRNA\n",
    "2. Base strain\n",
    "3. Library\n",
    "4. Phenotyping\n",
    "**5. Machine Learning**\n",
    "   - Predict the best promoter:gene combinations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the AutoML library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: java version \"1.8.0_361\"; Java(TM) SE Runtime Environment (build 1.8.0_361-b09); Java HotSpot(TM) 64-Bit Server VM (build 25.361-b09, mixed mode)\n",
      "  Starting server from /Users/lucaslevassor/opt/anaconda3/envs/constrain/lib/python3.8/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /var/folders/2f/lw3sfzbs7l7f_q1knzbtbwrr0000gp/T/tmpknkv2kwn\n",
      "  JVM stdout: /var/folders/2f/lw3sfzbs7l7f_q1knzbtbwrr0000gp/T/tmpknkv2kwn/h2o_lucaslevassor_started_from_python.out\n",
      "  JVM stderr: /var/folders/2f/lw3sfzbs7l7f_q1knzbtbwrr0000gp/T/tmpknkv2kwn/h2o_lucaslevassor_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n",
      "Warning: Your H2O cluster version is too old (6 months and 18 days)!Please download and install the latest version from http://h2o.ai/download/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>21 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Europe/Copenhagen</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.36.1.3</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>6 months and 18 days !!!</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_lucaslevassor_iv10f1</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>7.667 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>0</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>0</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.8.13 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------------\n",
       "H2O_cluster_uptime:         21 secs\n",
       "H2O_cluster_timezone:       Europe/Copenhagen\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.36.1.3\n",
       "H2O_cluster_version_age:    6 months and 18 days !!!\n",
       "H2O_cluster_name:           H2O_from_python_lucaslevassor_iv10f1\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    7.667 Gb\n",
       "H2O_cluster_total_cores:    0\n",
       "H2O_cluster_allowed_cores:  0\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.8.13 final\n",
       "--------------------------  ------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If this doesnt work - install java. \n",
    "# Start the H2O cluster (locally)\n",
    "h2o.init(ip=\"localhost\", min_mem_size_GB=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h2o.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import out dataframe to h20 object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model based on the target and the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Line_name</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>Amt_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yp49_A01</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.922793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yp49_C01</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.509123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yp49_D01</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.166871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yp49_E01</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.327489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yp49_F01</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>25.060934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>yp51_C12</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>yp51_D12</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.591185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>yp51_E12</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0.448644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>yp50_F05</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>13.391244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>yp50_E07</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>75.633039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Line_name  0  1  2   3   Amt_norm\n",
       "0    yp49_A01  1  2  5   1   0.922793\n",
       "1    yp49_C01  1  2  5   9   0.509123\n",
       "2    yp49_D01  2  4  7   3   0.166871\n",
       "3    yp49_E01  2  1  5   7   0.327489\n",
       "4    yp49_F01  3  3  6   1  25.060934\n",
       "..        ... .. .. ..  ..        ...\n",
       "162  yp51_C12  8  3  7   2   0.000000\n",
       "163  yp51_D12  8  2  7   6   0.591185\n",
       "164  yp51_E12  8  1  6  10   0.448644\n",
       "165  yp50_F05  6  1  8   2  13.391244\n",
       "166  yp50_E07  5  1  6   9  75.633039\n",
       "\n",
       "[167 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run ML with new input \n",
    "new_input_for_ml = pd.read_csv('ConStrain_on_google_colab/data/09-AutoML/input_to_ml/first_round/input_for_ml_1st_iteration_all_analytics.csv')\n",
    "new_input_for_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input_for_ml['default_rank'] = new_input_for_ml['Amt_norm'].rank()\n",
    "new_input_for_ml['max_rank'] = new_input_for_ml['Amt_norm'].rank(method='max')\n",
    "new_input_for_ml['NA_bottom'] = new_input_for_ml['Amt_norm'].rank(na_option='bottom')\n",
    "new_input_for_ml['pct_rank'] = new_input_for_ml['Amt_norm'].rank(pct=True)\n",
    "new_input_for_ml = new_input_for_ml.sort_values(by= 'max_rank', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Line_name</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>Amt_norm</th>\n",
       "      <th>default_rank</th>\n",
       "      <th>max_rank</th>\n",
       "      <th>NA_bottom</th>\n",
       "      <th>pct_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>yp50_D03</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>245.034275</td>\n",
       "      <td>167.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>yp50_G03</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>156.327285</td>\n",
       "      <td>166.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>0.994012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>yp50_E05</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>144.340857</td>\n",
       "      <td>165.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>0.988024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>yp51_A02</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>135.693498</td>\n",
       "      <td>164.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>0.982036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>yp50_A09</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>134.321746</td>\n",
       "      <td>163.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>0.976048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>yp50_E01</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.179641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>yp50_D11</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.179641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>yp49_E05</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.179641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>yp49_H05</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.179641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>yp49_B05</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.179641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Line_name  0  1  2   3    Amt_norm  default_rank  max_rank  NA_bottom  \\\n",
       "73   yp50_D03  5  2  6   3  245.034275         167.0     167.0      167.0   \n",
       "75   yp50_G03  6  2  6   5  156.327285         166.0     166.0      166.0   \n",
       "86   yp50_E05  5  1  6   8  144.340857         165.0     165.0      165.0   \n",
       "126  yp51_A02  7  2  6   1  135.693498         164.0     164.0      164.0   \n",
       "106  yp50_A09  4  2  6   3  134.321746         163.0     163.0      163.0   \n",
       "..        ... .. .. ..  ..         ...           ...       ...        ...   \n",
       "60   yp50_E01  5  3  5   7    0.000000          30.0      59.0       30.0   \n",
       "114  yp50_D11  5  4  8   6    0.000000          30.0      59.0       30.0   \n",
       "24   yp49_E05  2  3  7   8    0.000000          30.0      59.0       30.0   \n",
       "27   yp49_H05  3  3  7  10    0.000000          30.0      59.0       30.0   \n",
       "22   yp49_B05  1  3  8   6    0.000000          30.0      59.0       30.0   \n",
       "\n",
       "     pct_rank  \n",
       "73   1.000000  \n",
       "75   0.994012  \n",
       "86   0.988024  \n",
       "126  0.982036  \n",
       "106  0.976048  \n",
       "..        ...  \n",
       "60   0.179641  \n",
       "114  0.179641  \n",
       "24   0.179641  \n",
       "27   0.179641  \n",
       "22   0.179641  \n",
       "\n",
       "[167 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_input_for_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing which coloums to train on \n",
    "new_input_for_ml = new_input_for_ml[['Line_name','0','1','2','3','Amt_norm']]\n",
    "new_input_for_ml.columns = ['Line_name','0','1','2','3','Amt_norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "Rows:167\n",
      "Cols:6\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>       </th><th>Line_name  </th><th>0                </th><th>1                 </th><th>2                 </th><th>3                </th><th>Amt_norm          </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>type   </td><td>string     </td><td>int              </td><td>int               </td><td>int               </td><td>int              </td><td>real              </td></tr>\n",
       "<tr><td>mins   </td><td>NaN        </td><td>1.0              </td><td>1.0               </td><td>5.0               </td><td>1.0              </td><td>0.0               </td></tr>\n",
       "<tr><td>mean   </td><td>NaN        </td><td>4.580838323353294</td><td>2.682634730538923 </td><td>6.562874251497007 </td><td>5.305389221556885</td><td>15.546391286319041</td></tr>\n",
       "<tr><td>maxs   </td><td>NaN        </td><td>8.0              </td><td>4.0               </td><td>8.0               </td><td>10.0             </td><td>245.03427469317091</td></tr>\n",
       "<tr><td>sigma  </td><td>NaN        </td><td>2.253055064641254</td><td>1.1621014428030096</td><td>1.1696200092422187</td><td>2.836323292129   </td><td>36.58610063884372 </td></tr>\n",
       "<tr><td>zeros  </td><td>0          </td><td>0                </td><td>0                 </td><td>0                 </td><td>0                </td><td>59                </td></tr>\n",
       "<tr><td>missing</td><td>0          </td><td>0                </td><td>0                 </td><td>0                 </td><td>0                </td><td>0                 </td></tr>\n",
       "<tr><td>0      </td><td>yp50_D03   </td><td>5.0              </td><td>2.0               </td><td>6.0               </td><td>3.0              </td><td>245.03427469317091</td></tr>\n",
       "<tr><td>1      </td><td>yp50_G03   </td><td>6.0              </td><td>2.0               </td><td>6.0               </td><td>5.0              </td><td>156.3272849244192 </td></tr>\n",
       "<tr><td>2      </td><td>yp50_E05   </td><td>5.0              </td><td>1.0               </td><td>6.0               </td><td>8.0              </td><td>144.34085698560642</td></tr>\n",
       "<tr><td>3      </td><td>yp51_A02   </td><td>7.0              </td><td>2.0               </td><td>6.0               </td><td>1.0              </td><td>135.6934975453943 </td></tr>\n",
       "<tr><td>4      </td><td>yp50_A09   </td><td>4.0              </td><td>2.0               </td><td>6.0               </td><td>3.0              </td><td>134.32174594528271</td></tr>\n",
       "<tr><td>5      </td><td>yp49_F04   </td><td>3.0              </td><td>2.0               </td><td>6.0               </td><td>2.0              </td><td>131.81169715123744</td></tr>\n",
       "<tr><td>6      </td><td>yp50_D07   </td><td>5.0              </td><td>2.0               </td><td>5.0               </td><td>5.0              </td><td>123.9098045873527 </td></tr>\n",
       "<tr><td>7      </td><td>yp51_A01   </td><td>7.0              </td><td>2.0               </td><td>5.0               </td><td>1.0              </td><td>122.21049426971835</td></tr>\n",
       "<tr><td>8      </td><td>yp50_F10   </td><td>6.0              </td><td>2.0               </td><td>5.0               </td><td>4.0              </td><td>100.4699002267856 </td></tr>\n",
       "<tr><td>9      </td><td>yp50_D05   </td><td>5.0              </td><td>2.0               </td><td>8.0               </td><td>4.0              </td><td>82.98283958228176 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test = h2o.H2OFrame(pd.concat([new_input_for_ml], axis='columns'))\n",
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the data is categorical we need to make it from numerical to categorical \n",
    "df_test['0']= df_test['0'].asfactor()\n",
    "df_test['1']= df_test['1'].asfactor()\n",
    "df_test['2'] = df_test['2'].asfactor()\n",
    "df_test['3'] = df_test['3'].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:167\n",
      "Cols:6\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>       </th><th>Line_name  </th><th>0   </th><th>1   </th><th>2   </th><th>3   </th><th>Amt_norm          </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>type   </td><td>string     </td><td>enum</td><td>enum</td><td>enum</td><td>enum</td><td>real              </td></tr>\n",
       "<tr><td>mins   </td><td>NaN        </td><td>    </td><td>    </td><td>    </td><td>    </td><td>0.0               </td></tr>\n",
       "<tr><td>mean   </td><td>NaN        </td><td>    </td><td>    </td><td>    </td><td>    </td><td>15.546391286319041</td></tr>\n",
       "<tr><td>maxs   </td><td>NaN        </td><td>    </td><td>    </td><td>    </td><td>    </td><td>245.03427469317091</td></tr>\n",
       "<tr><td>sigma  </td><td>NaN        </td><td>    </td><td>    </td><td>    </td><td>    </td><td>36.58610063884372 </td></tr>\n",
       "<tr><td>zeros  </td><td>0          </td><td>    </td><td>    </td><td>    </td><td>    </td><td>59                </td></tr>\n",
       "<tr><td>missing</td><td>0          </td><td>0   </td><td>0   </td><td>0   </td><td>0   </td><td>0                 </td></tr>\n",
       "<tr><td>0      </td><td>yp50_D03   </td><td>5   </td><td>2   </td><td>6   </td><td>3   </td><td>245.03427469317091</td></tr>\n",
       "<tr><td>1      </td><td>yp50_G03   </td><td>6   </td><td>2   </td><td>6   </td><td>5   </td><td>156.3272849244192 </td></tr>\n",
       "<tr><td>2      </td><td>yp50_E05   </td><td>5   </td><td>1   </td><td>6   </td><td>8   </td><td>144.34085698560642</td></tr>\n",
       "<tr><td>3      </td><td>yp51_A02   </td><td>7   </td><td>2   </td><td>6   </td><td>1   </td><td>135.6934975453943 </td></tr>\n",
       "<tr><td>4      </td><td>yp50_A09   </td><td>4   </td><td>2   </td><td>6   </td><td>3   </td><td>134.32174594528271</td></tr>\n",
       "<tr><td>5      </td><td>yp49_F04   </td><td>3   </td><td>2   </td><td>6   </td><td>2   </td><td>131.81169715123744</td></tr>\n",
       "<tr><td>6      </td><td>yp50_D07   </td><td>5   </td><td>2   </td><td>5   </td><td>5   </td><td>123.9098045873527 </td></tr>\n",
       "<tr><td>7      </td><td>yp51_A01   </td><td>7   </td><td>2   </td><td>5   </td><td>1   </td><td>122.21049426971835</td></tr>\n",
       "<tr><td>8      </td><td>yp50_F10   </td><td>6   </td><td>2   </td><td>5   </td><td>4   </td><td>100.4699002267856 </td></tr>\n",
       "<tr><td>9      </td><td>yp50_D05   </td><td>5   </td><td>2   </td><td>8   </td><td>4   </td><td>82.98283958228176 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some arguments for the autoML. \n",
    "\n",
    "\n",
    "Important here is that we dont split the dataset but rather keep the cross-validation validate a model internally, i.e., estimate the model performance without having to sacrifice a validation split. See: https://docs.h2o.ai/h2o/latest-stable/h2o-docs/cross-validation.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE: IF you don't want to run the autoML skip the next few paragraphs and go to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asdfasdfa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43masdfasdfa\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'asdfasdfa' is not defined"
     ]
    }
   ],
   "source": [
    "asdfasdfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. RUN automl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are defining the autoML object and after we can train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns we want to train on\n",
    "feature_cols = ['0', '1', '2', '3']\n",
    "\n",
    "# Initialize H2O autoML class\n",
    "AutoML = H2OAutoML(\n",
    "    max_runtime_secs=0,  # 1 hour =int(3600 * 1) , if unlimited time is wanted then set this to zero = 0\n",
    "    max_models=None,  # None =  no limit\n",
    "    nfolds=10,         # number of folds for k-fold cross-validation (nfolds=0 disables cross-validation)\n",
    "    seed=1,            # Reproducibility\n",
    "#    exclude_algos = [\"StackedEnsemble\"],\n",
    "    sort_metric = \"MAE\",\n",
    "    keep_cross_validation_predictions=True \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model based on the target and the df. \n",
    "\n",
    "\n",
    "It has been Commented out and saved as showed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "AutoML.train(\n",
    "     x=feature_cols,\n",
    "     y='Amt_norm',\n",
    "     training_frame=df_test,\n",
    " ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DOOOOOOONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Processing model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the leaderboard ids\n",
    "leaderboard = AutoML.leaderboard\n",
    "model_ids = list(leaderboard['model_id'].as_data_frame().iloc[:,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to save any model\n",
    "out_path = 'ConStrain_on_google_colab/data/09-AutoML/best_models/first_round/'\n",
    "mdl = h2o.get_model(model_ids[0])\n",
    "h2o.save_model(model=mdl, path=out_path, force=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Saving the leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the leaderboard\n",
    "out_path = 'ConStrain_on_google_colab/data/09-AutoML/leaderboards/first_round/'\n",
    "h2o.export_file(leaderboard, os.path.join(out_path, 'aml_leaderboard.h2o'), force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the models\n",
    "#leaderboard = AutoML.leaderboard\n",
    "#lb= leaderboard\n",
    "#model_ids = list(leaderboard['model_id'].as_data_frame().iloc[:,0])\n",
    "#\n",
    "#out_path = 'ConStrain_on_google_colab/data/09-AutoML/leaderboards/first_round/'\n",
    "#for m_id in model_ids:\n",
    "#     mdl = h2o.get_model(m_id)\n",
    "#     h2o.save_model(model=mdl, path=out_path, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to extract the best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_h2o_object = leaderboard.as_data_frame(use_pandas=True, header=True)\n",
    "df_from_h2o_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model = AutoML.get_best_model()\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.4 saving the CV-holdout predictions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions\n",
    "cross_validation_holdout_predictions = best_model.cross_validation_holdout_predictions()\n",
    "\n",
    "# make the df_test to a dataframe\n",
    "as_data_frame_df_test = df_test.as_data_frame()\n",
    "\n",
    "# Make cv_pred to a dataframe\n",
    "as_data_frame_CV_predictions = cross_validation_holdout_predictions.as_data_frame()\n",
    "as_data_frame_CV_predictions.columns = ['cv_holdout_predictions']\n",
    "as_data_frame_CV_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with the test dataframe\n",
    "df3 = pd.merge(as_data_frame_df_test, as_data_frame_CV_predictions, left_index=True, right_index=True)\n",
    "df3 = df3.sort_values(by= 'Amt_norm', ascending= False)\n",
    "df3 = df3.reset_index()\n",
    "df3['ranked_pct'] = df3['Amt_norm'].rank(pct= True)\n",
    "df3['cv_holdout_predictions_ranked_pct'] = df3['cv_holdout_predictions'].rank(pct= True)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to the folder\n",
    "df3.to_csv('ConStrain_on_google_colab/data/09-AutoML/cv_holdout_predictions/first_round/cv_holdout_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Importing the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.1 Importing the leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = \"ConStrain_on_google_colab/data/09-AutoML/leaderboards/first_round/\"\n",
    "\n",
    "lb = h2o.import_file(path=os.path.join(models_path, \"aml_leaderboard.h2o\"))\n",
    "lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = h2o.load_model(\"ConStrain_on_google_colab/data/09-AutoML/best_models/first_round/\"+lb[0,0]) \n",
    "my_local_model = h2o.download_model(best_model, path=\"ConStrain_on_google_colab/data/09-AutoML/best_models/first_round/\")\n",
    "uploaded_model = h2o.upload_model(my_local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Investigate the best model - Cross-validation holdout predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the cross validation predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('ConStrain_on_google_colab/data/09-AutoML/cv_holdout_predictions/second_round/cv_holdout_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross_validation_holdout_predictions = best_model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where list of cv preds are stored (one element per fold):\n",
    "#cross_validation_holdout_predictions = best_model.cross_validation_holdout_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross_validation_holdout_predictions.frame_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv_fram = cross_validation_holdout_predictions.as_data_frame()\n",
    "#cv_fram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However you most likely want a single-column frame including all cv preds\n",
    "#cross_validation_predictions = best_model.cross_validation_predictions()\n",
    "#print(len(cross_validation_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as_data_frame_df_test = df_test.as_data_frame()\n",
    "#as_data_frame_CV_predictions = cross_validation_holdout_predictions.as_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.merge(as_data_frame_df_test, as_data_frame_CV_predictions, left_index=True, right_index=True).sort_values(by= 'Amt_norm', ascending = False).reset_index()\n",
    "df3.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Barplot of production vs prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constrain.plotting.plotting import bar_plot_w_hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# initialize\n",
    "ds1 = df3[['Line_name', 'Amt_norm']]\n",
    "ds1.columns = ['Line_name', 'strict']\n",
    "ds2 = df3[['Line_name', 'cv_holdout_predictions']]\n",
    "ds2.columns = ['Line_name', 'strict']\n",
    "\n",
    "# add category\n",
    "ds2['category'] = 'Predicted'\n",
    "ds1['category'] = 'Strictosidine'\n",
    "dss = pd.concat([ds1, ds2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot_w_hue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_plot_w_hue(dss, 'Line_name', 'strict', \n",
    "               path = 'ConStrain_on_google_colab/data/10-plots/07_1_LEARN_DataAnalysis/Prediction of the sampled library_ranked',\n",
    "               palette = 'dark',\n",
    "              size_height= 10, \n",
    "              size_length = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking production and correlation plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['norm_strict_measured_rank_pct']= df3['Amt_norm'].rank(pct=True)\n",
    "df3['Predicted_strict_production_rank_pct'] = df3['cv_holdout_predictions'].rank(pct=True)\n",
    "df3.to_csv('ConStrain_on_google_colab/data/09-AutoML/all_predictions/first_round/input_for_ml_1st_iteration_w_predictions_and_ranking_2701.csv')\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constrain.plotting.plotting import correlation_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_plot(df3,\"Amt_norm\",\"cv_holdout_predictions\", save_pdf = True , \n",
    "                 path ='ConStrain_on_google_colab/data/10-plots/07_1_LEARN_DataAnalysis/corr_plot_Amt_predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_plot(df3,\"norm_strict_measured_rank_pct\",\"Predicted_strict_production_rank_pct\", save_pdf = True , \n",
    "                 path ='ConStrain_on_google_colab/data/10-plots/07_1_LEARN_DataAnalysis/corr_plot_rank_rank_predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Clean up data on the remaining library of combinations of genotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genotypes = pd.read_csv('ConStrain_on_google_colab/data/03-strain_sequences/systematic_names_of_all_strains/systematic_names_on_all_combinations.csv')\n",
    "all_genotypes_df = h2o.H2OFrame(pd.concat([all_genotypes], axis='columns'))\n",
    "all_genotypes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fixed = new_input_for_ml[['0','1','2','3']]\n",
    "input_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting rid of the genotypes we have already tested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff = all_genotypes[~all_genotypes.apply(tuple,1).isin(input_fixed.apply(tuple,1))]\n",
    "df_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the diff into a df \n",
    "all_genotypes_df = h2o.H2OFrame(pd.concat([df_diff], axis='columns'))\n",
    "all_genotypes_df['0']= all_genotypes_df['0'].asfactor()\n",
    "all_genotypes_df['1']= all_genotypes_df['1'].asfactor()\n",
    "all_genotypes_df['2'] = all_genotypes_df['2'].asfactor()\n",
    "all_genotypes_df['3'] = all_genotypes_df['3'].asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predict the  phenotypes from the whole library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = best_model.predict(all_genotypes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_norm = predicted.as_data_frame()\n",
    "predicted_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions into a list\n",
    "predicted_list = predicted_norm['predict'].values.tolist()\n",
    "\n",
    "# Adding predictions to our dataframe\n",
    "df_diff['predicted_norm_strict'] = predicted_list\n",
    "\n",
    "# Sorting the dataframe\n",
    "predicted_merged_sorted = df_diff.sort_values('predicted_norm_strict', ascending=False)\n",
    "predicted_merged_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want the names on the genotypes we can do the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g8h_list_of_index = [\"Cacu\", \"Opum\",\"Cro\",\"Vmin\",\"Smus\",\"Rsep\",\"Oeu\",\"Ccal\"]\n",
    "cpr_list_of_index = [\"Cro\", \"Aan\",\"Ara\",\"Clo\",\"Rse\",\"Ahu\",\"Ani\",\"Cac\",\"Oeu\",\"Cpo\"]\n",
    "promoters = [\"CYC1\", \"ENO2\",\"PCK1\",\"RPL15B\", \"CCW12\", \"TPI1\",\"MLS1\",\"URE2\"]\n",
    "\n",
    "g8h_genotype = []\n",
    "cpr_genotype = []\n",
    "\n",
    "pg8h_genotype = []\n",
    "pcpr_genotype = []\n",
    "\n",
    "predicted_list = []\n",
    "# adding index of genotypes to individual rows \n",
    "for index, row in predicted_merged_sorted.iterrows():\n",
    "    g8h_genotype.append(g8h_list_of_index[int(row['0'])-1])\n",
    "    cpr_genotype.append(cpr_list_of_index[int(row['3'])-1])\n",
    "    pg8h_genotype.append(promoters[int(row['1'])-1])\n",
    "    pcpr_genotype.append(promoters[int(row['2'])-1])\n",
    "    predicted_list.append(row['predicted_norm_strict'])\n",
    "    \n",
    "    \n",
    "list_of_lists = [g8h_genotype,pg8h_genotype,pcpr_genotype, cpr_genotype, predicted_list  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_genotypes = pd.DataFrame(list_of_lists )\n",
    "df_with_genotypes = df_with_genotypes.T\n",
    "df_with_genotypes.columns = ['G8H', 'pG8H', 'pCPR', 'CPR', 'Predicted_strict_production']\n",
    "df_with_genotypes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constrain.plotting.plotting import bar_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Plot of predicted strictosidine production across all promoter:homolog combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(df_with_genotypes.index)\n",
    "y = list(df_with_genotypes['Predicted_strict_production'])\n",
    "\n",
    "bar_plot(x, y, path = 'ConStrain_on_google_colab/data/10-plots/07_1_LEARN_DataAnalysis/barplot_predicting_remaining_best_combinations_2701')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_genotypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save the dataframes into csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a unique name\n",
    "from datetime import datetime\n",
    "now = datetime.now() # current date and time\n",
    "time = now.strftime(\"%Y_%m_%d_%H:%M_\")\n",
    "\n",
    "name = 'top_ML_predicted_after_first_DBTL_merged_analytics'\n",
    "path = 'ConStrain_on_google_colab/data/09-AutoML/all_predictions/first_round/'\n",
    "\n",
    "df_with_genotypes.to_csv(path+time+name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Top 20 predicted producers of the unseen library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the file generated first from one of the first models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_genotypes = pd.read_csv('ConStrain_on_google_colab/data/09-AutoML/all_predictions/first_round/top_ML_predicted_after_first_DBTL_merged_analytics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_genotypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding unique names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_genotypes['names'] = df_with_genotypes['G8H']+'-' +df_with_genotypes['pG8H']+ '_'+ df_with_genotypes['pCPR']+'_'+df_with_genotypes['CPR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constrain.plotting.plotting import horisontal_bar_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting top 20 strains\n",
    "x_axis = list(df_with_genotypes['names'])[:20][::-1]\n",
    "y_axis = list(df_with_genotypes['Predicted_strict_production'])[:20][::-1]\n",
    "\n",
    "horisontal_bar_plot(x_axis,y_axis, path = 'ConStrain_on_google_colab/data/10-plots/07_1_LEARN_DataAnalysis/Top20 predicted strains DBTL1', \n",
    "                    title = 'Top20 predicted strains DBTL1', \n",
    "                   size_height=5, \n",
    "                   size_length=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 How many genotypes are predicted to produce more than the control strain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_100_strict_procduction = df_with_genotypes[df_with_genotypes['Predicted_strict_production'] >= 100 ]\n",
    "print(f\"{len(over_100_strict_procduction)} strains out of {len(df_with_genotypes)} showed production of strictosidine over the normalized value\")\n",
    "print(f\"Out of the remaing theoretical space these constittue : {(len(over_100_strict_procduction)/len(df_with_genotypes))*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8. Learning curve on partitioned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constrain.machine_learning.auto_ml import autoML_on_partitioned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_new_input_for_ml = new_input_for_ml.sample(frac=1, random_state= 2).reset_index(drop=True) # Random state sets a seed on the shuffeling\n",
    "shuffled_new_input_for_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already did this. It takes +20 hours on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_cols = ['0', '1', '2', '3']\n",
    "#training_column = 'Amt_norm'\n",
    "#\n",
    "## Training 3 partitions 5 times - the function is set to nfold = 10  by default. \n",
    "#for i in range(0,3): \n",
    "#    autoML_on_partitioned_data(feature_cols, training_column,\n",
    "#                               new_input_for_ml,\n",
    "#                               training_time=0, \n",
    "#                               partitions = 3,\n",
    "#                               nfold= 10,\n",
    "#                               path = 'ConStrain_on_google_colab/data/09-AutoML/learning_curve_data/first_round/0sec_experiment/NOT_shuffled_60sec/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 - visualizing learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# folder path\n",
    "dir_path = 'ConStrain_on_google_colab/data/09-AutoML/learning_curve_data/first_round/nfold_10/0sec_experiment/shuffled_random2/'\n",
    "\n",
    "# list file and directories\n",
    "res = os.listdir(dir_path)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_of_pd_dataframes = []\n",
    "for ml_partitioned in res:\n",
    "    if ml_partitioned.endswith('.csv'):\n",
    "        lst_of_pd_dataframes.append(pd.read_csv(dir_path+ml_partitioned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets retrieve the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df.set_index('Unnamed: 0') for df in lst_of_pd_dataframes]\n",
    "concated = pd.concat(dfs, axis=1)\n",
    "concated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test results\n",
    "test_results = concated[['0']]\n",
    "test_results['mean'] = test_results.mean(axis=1)\n",
    "test_results['std'] = test_results.std(axis=1)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated mean MAE\n",
    "cross_validated_results = concated[['CV_mean_MAE']]\n",
    "cross_validated_results['mean'] = cross_validated_results.mean(axis=1)\n",
    "cross_validated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated mean SD\n",
    "cross_validated_results_sd = concated[['CV_SD_MAE']]\n",
    "cross_validated_results_sd['mean'] = cross_validated_results_sd.mean(axis=1)\n",
    "cross_validated_results_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constrain.plotting.plotting import plot_ml_learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV\n",
    "x = list(cross_validated_results.index)\n",
    "y = list(cross_validated_results['mean'])\n",
    "sd =  list(cross_validated_results_sd['mean'])\n",
    "\n",
    "# model_performance\n",
    "y1 = list(test_results['mean'])\n",
    "sd1 = np.array(list(test_results['std']))\n",
    "\n",
    "\n",
    "plot_ml_learning_curve(x, y1, y,sd1,  sd, save_pdf = True ,\n",
    "                       path = 'ConStrain_on_google_colab/data/10-plots/07_1_LEARN_DataAnalysis/Learning_curve_on_partitioned_data_DBTL1_12_01_2023_10_height_10_length', \n",
    "                            size_height = 10,\n",
    "                             size_length = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyODiKHEQc6jrygVbFvE0AA+",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "08-LEARN-DataAnalysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
